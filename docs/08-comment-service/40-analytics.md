# ç¬¬40æ­¥ï¼šç»Ÿè®¡åˆ†æåŠŸèƒ½

## ğŸ“‹ æ¦‚è¿°

ç»Ÿè®¡åˆ†æåŠŸèƒ½ä¸ºMovieInfoé¡¹ç›®æä¾›å…¨é¢çš„æ•°æ®æ´å¯Ÿå’Œè¿è¥æ”¯æŒã€‚é€šè¿‡æ”¶é›†ã€å¤„ç†å’Œåˆ†æç”¨æˆ·è¡Œä¸ºã€å†…å®¹è´¨é‡ã€å¹³å°å¥åº·åº¦ç­‰å¤šç»´åº¦æ•°æ®ï¼Œä¸ºäº§å“ä¼˜åŒ–å’Œè¿è¥å†³ç­–æä¾›ç§‘å­¦ä¾æ®ã€‚

## ğŸ¯ è®¾è®¡ç›®æ ‡

### 1. **æ•°æ®å®Œæ•´æ€§**
- å…¨é¢çš„æ•°æ®æ”¶é›†
- å¤šç»´åº¦æŒ‡æ ‡ä½“ç³»
- å®æ—¶æ•°æ®æ›´æ–°
- å†å²æ•°æ®è¿½è¸ª

### 2. **åˆ†ææ·±åº¦**
- ç”¨æˆ·è¡Œä¸ºåˆ†æ
- å†…å®¹è´¨é‡åˆ†æ
- è¶‹åŠ¿é¢„æµ‹åˆ†æ
- å¼‚å¸¸æ£€æµ‹åˆ†æ

### 3. **å¯è§†åŒ–å±•ç¤º**
- ç›´è§‚çš„å›¾è¡¨å±•ç¤º
- äº¤äº’å¼æ•°æ®æ¢ç´¢
- è‡ªå®šä¹‰æŠ¥è¡¨ç”Ÿæˆ
- ç§»åŠ¨ç«¯é€‚é…

## ğŸ”§ ç»Ÿè®¡åˆ†ææ¶æ„

### 1. **æ•°æ®æ¨¡å‹è®¾è®¡**

```go
// ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡
type UserBehaviorStats struct {
    ID              string    `gorm:"primaryKey" json:"id"`
    UserID          string    `gorm:"not null;index" json:"user_id"`
    Date            time.Time `gorm:"not null;index" json:"date"`
    
    // æ´»è·ƒåº¦æŒ‡æ ‡
    LoginCount      int       `gorm:"not null;default:0" json:"login_count"`
    PageViews       int       `gorm:"not null;default:0" json:"page_views"`
    SessionDuration int       `gorm:"not null;default:0" json:"session_duration"` // ç§’
    
    // å†…å®¹äº’åŠ¨
    CommentsPosted  int       `gorm:"not null;default:0" json:"comments_posted"`
    RatingsGiven    int       `gorm:"not null;default:0" json:"ratings_given"`
    LikesGiven      int       `gorm:"not null;default:0" json:"likes_given"`
    SharesCount     int       `gorm:"not null;default:0" json:"shares_count"`
    
    // æœç´¢è¡Œä¸º
    SearchQueries   int       `gorm:"not null;default:0" json:"search_queries"`
    MoviesViewed    int       `gorm:"not null;default:0" json:"movies_viewed"`
    
    CreatedAt       time.Time `json:"created_at"`
    UpdatedAt       time.Time `json:"updated_at"`
}

// å†…å®¹ç»Ÿè®¡
type ContentStats struct {
    ID              string    `gorm:"primaryKey" json:"id"`
    ContentID       string    `gorm:"not null;index" json:"content_id"`
    ContentType     string    `gorm:"not null;size:50" json:"content_type"` // movie, comment, rating
    Date            time.Time `gorm:"not null;index" json:"date"`
    
    // æµè§ˆç»Ÿè®¡
    ViewCount       int       `gorm:"not null;default:0" json:"view_count"`
    UniqueViews     int       `gorm:"not null;default:0" json:"unique_views"`
    
    // äº’åŠ¨ç»Ÿè®¡
    LikeCount       int       `gorm:"not null;default:0" json:"like_count"`
    CommentCount    int       `gorm:"not null;default:0" json:"comment_count"`
    ShareCount      int       `gorm:"not null;default:0" json:"share_count"`
    
    // è¯„åˆ†ç»Ÿè®¡
    RatingCount     int       `gorm:"not null;default:0" json:"rating_count"`
    AverageRating   float64   `gorm:"not null;default:0" json:"average_rating"`
    
    // è´¨é‡æŒ‡æ ‡
    QualityScore    float64   `gorm:"not null;default:0" json:"quality_score"`
    EngagementRate  float64   `gorm:"not null;default:0" json:"engagement_rate"`
    
    CreatedAt       time.Time `json:"created_at"`
    UpdatedAt       time.Time `json:"updated_at"`
}

// å¹³å°ç»Ÿè®¡
type PlatformStats struct {
    ID              string    `gorm:"primaryKey" json:"id"`
    Date            time.Time `gorm:"not null;uniqueIndex" json:"date"`
    
    // ç”¨æˆ·æŒ‡æ ‡
    TotalUsers      int       `gorm:"not null;default:0" json:"total_users"`
    ActiveUsers     int       `gorm:"not null;default:0" json:"active_users"`
    NewUsers        int       `gorm:"not null;default:0" json:"new_users"`
    RetentionRate   float64   `gorm:"not null;default:0" json:"retention_rate"`
    
    // å†…å®¹æŒ‡æ ‡
    TotalMovies     int       `gorm:"not null;default:0" json:"total_movies"`
    TotalComments   int       `gorm:"not null;default:0" json:"total_comments"`
    TotalRatings    int       `gorm:"not null;default:0" json:"total_ratings"`
    
    // æ´»è·ƒåº¦æŒ‡æ ‡
    PageViews       int       `gorm:"not null;default:0" json:"page_views"`
    Sessions        int       `gorm:"not null;default:0" json:"sessions"`
    AvgSessionTime  int       `gorm:"not null;default:0" json:"avg_session_time"`
    
    // è´¨é‡æŒ‡æ ‡
    ContentQuality  float64   `gorm:"not null;default:0" json:"content_quality"`
    UserSatisfaction float64  `gorm:"not null;default:0" json:"user_satisfaction"`
    
    CreatedAt       time.Time `json:"created_at"`
    UpdatedAt       time.Time `json:"updated_at"`
}

// ç»Ÿè®¡æŸ¥è¯¢è¯·æ±‚
type AnalyticsRequest struct {
    MetricType  string    `json:"metric_type" binding:"required"` // user, content, platform
    StartDate   time.Time `json:"start_date" binding:"required"`
    EndDate     time.Time `json:"end_date" binding:"required"`
    Granularity string    `json:"granularity"` // hour, day, week, month
    Filters     map[string]interface{} `json:"filters,omitempty"`
    GroupBy     []string  `json:"group_by,omitempty"`
}

// ç»Ÿè®¡å“åº”
type AnalyticsResponse struct {
    Success   bool                   `json:"success"`
    Message   string                 `json:"message,omitempty"`
    Data      []AnalyticsDataPoint   `json:"data,omitempty"`
    Summary   *AnalyticsSummary      `json:"summary,omitempty"`
    Metadata  *AnalyticsMetadata     `json:"metadata,omitempty"`
}

type AnalyticsDataPoint struct {
    Timestamp time.Time              `json:"timestamp"`
    Metrics   map[string]interface{} `json:"metrics"`
    Dimensions map[string]string     `json:"dimensions,omitempty"`
}

type AnalyticsSummary struct {
    TotalRecords int                    `json:"total_records"`
    Aggregations map[string]interface{} `json:"aggregations"`
    Trends       map[string]float64     `json:"trends"`
}

type AnalyticsMetadata struct {
    QueryTime    time.Duration `json:"query_time_ms"`
    CacheHit     bool          `json:"cache_hit"`
    DataSource   string        `json:"data_source"`
    LastUpdated  time.Time     `json:"last_updated"`
}
```

### 2. **ç»Ÿè®¡åˆ†ææœåŠ¡**

```go
type AnalyticsService struct {
    userStatsRepo     UserBehaviorStatsRepository
    contentStatsRepo  ContentStatsRepository
    platformStatsRepo PlatformStatsRepository
    cacheStore        CacheStore
    timeSeriesDB      TimeSeriesDB
    logger            *logrus.Logger
    metrics           *AnalyticsMetrics
}

func NewAnalyticsService(
    userStatsRepo UserBehaviorStatsRepository,
    contentStatsRepo ContentStatsRepository,
    platformStatsRepo PlatformStatsRepository,
    cacheStore CacheStore,
    timeSeriesDB TimeSeriesDB,
) *AnalyticsService {
    return &AnalyticsService{
        userStatsRepo:     userStatsRepo,
        contentStatsRepo:  contentStatsRepo,
        platformStatsRepo: platformStatsRepo,
        cacheStore:        cacheStore,
        timeSeriesDB:      timeSeriesDB,
        logger:            logrus.New(),
        metrics:           NewAnalyticsMetrics(),
    }
}

// è·å–åˆ†ææ•°æ®
func (as *AnalyticsService) GetAnalytics(ctx context.Context, req *AnalyticsRequest) (*AnalyticsResponse, error) {
    start := time.Now()
    defer func() {
        as.metrics.ObserveQueryDuration(req.MetricType, time.Since(start))
    }()

    // éªŒè¯è¯·æ±‚å‚æ•°
    if err := as.validateRequest(req); err != nil {
        as.metrics.IncInvalidRequests()
        return &AnalyticsResponse{
            Success: false,
            Message: err.Error(),
        }, nil
    }

    // ç”Ÿæˆç¼“å­˜é”®
    cacheKey := as.generateCacheKey(req)

    // å°è¯•ä»ç¼“å­˜è·å–
    if cachedResult, err := as.getFromCache(ctx, cacheKey); err == nil {
        as.metrics.IncCacheHits()
        return cachedResult, nil
    }
    as.metrics.IncCacheMisses()

    // æ ¹æ®æŒ‡æ ‡ç±»å‹æ‰§è¡ŒæŸ¥è¯¢
    var data []AnalyticsDataPoint
    var err error

    switch req.MetricType {
    case "user":
        data, err = as.getUserAnalytics(ctx, req)
    case "content":
        data, err = as.getContentAnalytics(ctx, req)
    case "platform":
        data, err = as.getPlatformAnalytics(ctx, req)
    default:
        as.metrics.IncInvalidRequests()
        return &AnalyticsResponse{
            Success: false,
            Message: "ä¸æ”¯æŒçš„æŒ‡æ ‡ç±»å‹",
        }, nil
    }

    if err != nil {
        as.logger.Errorf("Failed to get analytics data: %v", err)
        as.metrics.IncQueryErrors()
        return nil, errors.New("è·å–åˆ†ææ•°æ®å¤±è´¥")
    }

    // è®¡ç®—æ±‡æ€»ä¿¡æ¯
    summary := as.calculateSummary(data, req)

    // æ„å»ºå“åº”
    response := &AnalyticsResponse{
        Success: true,
        Data:    data,
        Summary: summary,
        Metadata: &AnalyticsMetadata{
            QueryTime:   time.Since(start),
            CacheHit:    false,
            DataSource:  "database",
            LastUpdated: time.Now(),
        },
    }

    // å¼‚æ­¥ç¼“å­˜ç»“æœ
    go func() {
        if err := as.cacheResult(context.Background(), cacheKey, response); err != nil {
            as.logger.Errorf("Failed to cache analytics result: %v", err)
        }
    }()

    as.metrics.IncSuccessfulQueries()
    return response, nil
}

// è·å–ç”¨æˆ·è¡Œä¸ºåˆ†æ
func (as *AnalyticsService) getUserAnalytics(ctx context.Context, req *AnalyticsRequest) ([]AnalyticsDataPoint, error) {
    // æ„å»ºæŸ¥è¯¢æ¡ä»¶
    queryBuilder := as.buildUserStatsQuery(req)

    // æ‰§è¡ŒæŸ¥è¯¢
    stats, err := as.userStatsRepo.FindWithQuery(ctx, queryBuilder)
    if err != nil {
        return nil, err
    }

    // è½¬æ¢ä¸ºæ•°æ®ç‚¹
    dataPoints := make([]AnalyticsDataPoint, 0)
    
    // æŒ‰æ—¶é—´åˆ†ç»„èšåˆ
    groupedData := as.groupUserStatsByTime(stats, req.Granularity)
    
    for timestamp, groupStats := range groupedData {
        metrics := map[string]interface{}{
            "active_users":     len(groupStats),
            "total_page_views": as.sumUserMetric(groupStats, "page_views"),
            "total_comments":   as.sumUserMetric(groupStats, "comments_posted"),
            "total_ratings":    as.sumUserMetric(groupStats, "ratings_given"),
            "avg_session_time": as.avgUserMetric(groupStats, "session_duration"),
        }

        dataPoints = append(dataPoints, AnalyticsDataPoint{
            Timestamp: timestamp,
            Metrics:   metrics,
        })
    }

    // æŒ‰æ—¶é—´æ’åº
    sort.Slice(dataPoints, func(i, j int) bool {
        return dataPoints[i].Timestamp.Before(dataPoints[j].Timestamp)
    })

    return dataPoints, nil
}

// è·å–å†…å®¹åˆ†æ
func (as *AnalyticsService) getContentAnalytics(ctx context.Context, req *AnalyticsRequest) ([]AnalyticsDataPoint, error) {
    queryBuilder := as.buildContentStatsQuery(req)

    stats, err := as.contentStatsRepo.FindWithQuery(ctx, queryBuilder)
    if err != nil {
        return nil, err
    }

    dataPoints := make([]AnalyticsDataPoint, 0)
    groupedData := as.groupContentStatsByTime(stats, req.Granularity)

    for timestamp, groupStats := range groupedData {
        metrics := map[string]interface{}{
            "total_views":      as.sumContentMetric(groupStats, "view_count"),
            "unique_views":     as.sumContentMetric(groupStats, "unique_views"),
            "total_likes":      as.sumContentMetric(groupStats, "like_count"),
            "total_comments":   as.sumContentMetric(groupStats, "comment_count"),
            "avg_rating":       as.avgContentMetric(groupStats, "average_rating"),
            "engagement_rate":  as.avgContentMetric(groupStats, "engagement_rate"),
        }

        dataPoints = append(dataPoints, AnalyticsDataPoint{
            Timestamp: timestamp,
            Metrics:   metrics,
        })
    }

    sort.Slice(dataPoints, func(i, j int) bool {
        return dataPoints[i].Timestamp.Before(dataPoints[j].Timestamp)
    })

    return dataPoints, nil
}

// è·å–å¹³å°åˆ†æ
func (as *AnalyticsService) getPlatformAnalytics(ctx context.Context, req *AnalyticsRequest) ([]AnalyticsDataPoint, error) {
    stats, err := as.platformStatsRepo.FindByDateRange(ctx, req.StartDate, req.EndDate)
    if err != nil {
        return nil, err
    }

    dataPoints := make([]AnalyticsDataPoint, len(stats))
    for i, stat := range stats {
        dataPoints[i] = AnalyticsDataPoint{
            Timestamp: stat.Date,
            Metrics: map[string]interface{}{
                "total_users":       stat.TotalUsers,
                "active_users":      stat.ActiveUsers,
                "new_users":         stat.NewUsers,
                "retention_rate":    stat.RetentionRate,
                "total_movies":      stat.TotalMovies,
                "total_comments":    stat.TotalComments,
                "total_ratings":     stat.TotalRatings,
                "page_views":        stat.PageViews,
                "sessions":          stat.Sessions,
                "avg_session_time":  stat.AvgSessionTime,
                "content_quality":   stat.ContentQuality,
                "user_satisfaction": stat.UserSatisfaction,
            },
        }
    }

    return dataPoints, nil
}

// è®¡ç®—æ±‡æ€»ä¿¡æ¯
func (as *AnalyticsService) calculateSummary(data []AnalyticsDataPoint, req *AnalyticsRequest) *AnalyticsSummary {
    if len(data) == 0 {
        return &AnalyticsSummary{
            TotalRecords: 0,
            Aggregations: make(map[string]interface{}),
            Trends:       make(map[string]float64),
        }
    }

    aggregations := make(map[string]interface{})
    trends := make(map[string]float64)

    // è®¡ç®—èšåˆå€¼
    for metricName := range data[0].Metrics {
        values := make([]float64, len(data))
        for i, point := range data {
            if val, ok := point.Metrics[metricName].(float64); ok {
                values[i] = val
            } else if val, ok := point.Metrics[metricName].(int); ok {
                values[i] = float64(val)
            }
        }

        aggregations[metricName+"_sum"] = as.sum(values)
        aggregations[metricName+"_avg"] = as.average(values)
        aggregations[metricName+"_max"] = as.max(values)
        aggregations[metricName+"_min"] = as.min(values)

        // è®¡ç®—è¶‹åŠ¿ï¼ˆç®€å•çº¿æ€§å›å½’æ–œç‡ï¼‰
        trends[metricName] = as.calculateTrend(values)
    }

    return &AnalyticsSummary{
        TotalRecords: len(data),
        Aggregations: aggregations,
        Trends:       trends,
    }
}

// å®æ—¶æ•°æ®æ”¶é›†
func (as *AnalyticsService) RecordUserBehavior(ctx context.Context, userID string, action string, metadata map[string]interface{}) error {
    // è·å–æˆ–åˆ›å»ºä»Šæ—¥ç»Ÿè®¡è®°å½•
    today := time.Now().Truncate(24 * time.Hour)
    stats, err := as.userStatsRepo.FindByUserAndDate(ctx, userID, today)
    if err != nil && !errors.Is(err, gorm.ErrRecordNotFound) {
        return err
    }

    if stats == nil {
        stats = &UserBehaviorStats{
            ID:     uuid.New().String(),
            UserID: userID,
            Date:   today,
        }
    }

    // æ ¹æ®è¡Œä¸ºç±»å‹æ›´æ–°ç»Ÿè®¡
    switch action {
    case "login":
        stats.LoginCount++
    case "page_view":
        stats.PageViews++
        if duration, ok := metadata["session_duration"].(int); ok {
            stats.SessionDuration += duration
        }
    case "comment_posted":
        stats.CommentsPosted++
    case "rating_given":
        stats.RatingsGiven++
    case "like_given":
        stats.LikesGiven++
    case "share":
        stats.SharesCount++
    case "search":
        stats.SearchQueries++
    case "movie_viewed":
        stats.MoviesViewed++
    }

    stats.UpdatedAt = time.Now()

    // ä¿å­˜æˆ–æ›´æ–°ç»Ÿè®¡
    if stats.CreatedAt.IsZero() {
        stats.CreatedAt = time.Now()
        return as.userStatsRepo.Create(ctx, stats)
    } else {
        return as.userStatsRepo.Update(ctx, stats)
    }
}

// æ‰¹é‡ç»Ÿè®¡æ›´æ–°
func (as *AnalyticsService) UpdateDailyStats(ctx context.Context) error {
    today := time.Now().Truncate(24 * time.Hour)
    
    // æ›´æ–°å¹³å°ç»Ÿè®¡
    if err := as.updatePlatformStats(ctx, today); err != nil {
        as.logger.Errorf("Failed to update platform stats: %v", err)
        return err
    }

    // æ›´æ–°å†…å®¹ç»Ÿè®¡
    if err := as.updateContentStats(ctx, today); err != nil {
        as.logger.Errorf("Failed to update content stats: %v", err)
        return err
    }

    as.logger.Infof("Daily stats updated for %s", today.Format("2006-01-02"))
    return nil
}

// æ•°æ®å¯¼å‡º
func (as *AnalyticsService) ExportData(ctx context.Context, req *AnalyticsRequest, format string) ([]byte, error) {
    // è·å–æ•°æ®
    response, err := as.GetAnalytics(ctx, req)
    if err != nil {
        return nil, err
    }

    switch format {
    case "csv":
        return as.exportToCSV(response.Data)
    case "json":
        return json.Marshal(response)
    case "excel":
        return as.exportToExcel(response.Data)
    default:
        return nil, errors.New("ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼")
    }
}
```

### 3. **å®æ—¶æ•°æ®æ”¶é›†**

```go
type EventCollector struct {
    analyticsService *AnalyticsService
    eventQueue       chan *Event
    batchSize        int
    flushInterval    time.Duration
    logger           *logrus.Logger
}

type Event struct {
    UserID    string                 `json:"user_id"`
    Action    string                 `json:"action"`
    Timestamp time.Time              `json:"timestamp"`
    Metadata  map[string]interface{} `json:"metadata"`
}

func NewEventCollector(analyticsService *AnalyticsService) *EventCollector {
    ec := &EventCollector{
        analyticsService: analyticsService,
        eventQueue:       make(chan *Event, 10000),
        batchSize:        100,
        flushInterval:    10 * time.Second,
        logger:           logrus.New(),
    }

    // å¯åŠ¨äº‹ä»¶å¤„ç†å™¨
    go ec.startEventProcessor()

    return ec
}

// æ”¶é›†äº‹ä»¶
func (ec *EventCollector) CollectEvent(userID, action string, metadata map[string]interface{}) {
    event := &Event{
        UserID:    userID,
        Action:    action,
        Timestamp: time.Now(),
        Metadata:  metadata,
    }

    select {
    case ec.eventQueue <- event:
        // äº‹ä»¶å·²å…¥é˜Ÿ
    default:
        // é˜Ÿåˆ—æ»¡ï¼Œä¸¢å¼ƒäº‹ä»¶
        ec.logger.Warn("Event queue full, dropping event")
    }
}

// äº‹ä»¶å¤„ç†å™¨
func (ec *EventCollector) startEventProcessor() {
    ticker := time.NewTicker(ec.flushInterval)
    defer ticker.Stop()

    var batch []*Event

    for {
        select {
        case event := <-ec.eventQueue:
            batch = append(batch, event)
            if len(batch) >= ec.batchSize {
                ec.processBatch(batch)
                batch = batch[:0]
            }

        case <-ticker.C:
            if len(batch) > 0 {
                ec.processBatch(batch)
                batch = batch[:0]
            }
        }
    }
}

// æ‰¹é‡å¤„ç†äº‹ä»¶
func (ec *EventCollector) processBatch(events []*Event) {
    ctx := context.Background()

    for _, event := range events {
        if err := ec.analyticsService.RecordUserBehavior(ctx, event.UserID, event.Action, event.Metadata); err != nil {
            ec.logger.Errorf("Failed to record user behavior: %v", err)
        }
    }

    ec.logger.Debugf("Processed %d events", len(events))
}
```

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

### 1. **åˆ†æç³»ç»ŸæŒ‡æ ‡**

```go
type AnalyticsMetrics struct {
    queryCount     *prometheus.CounterVec
    queryDuration  *prometheus.HistogramVec
    cacheHitRate   *prometheus.CounterVec
    dataPoints     prometheus.Gauge
    eventCount     *prometheus.CounterVec
}

func NewAnalyticsMetrics() *AnalyticsMetrics {
    return &AnalyticsMetrics{
        queryCount: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "analytics_queries_total",
                Help: "Total number of analytics queries",
            },
            []string{"metric_type", "status"},
        ),
        queryDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "analytics_query_duration_seconds",
                Help: "Duration of analytics queries",
            },
            []string{"metric_type"},
        ),
        cacheHitRate: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "analytics_cache_operations_total",
                Help: "Total number of analytics cache operations",
            },
            []string{"type"},
        ),
        dataPoints: prometheus.NewGauge(
            prometheus.GaugeOpts{
                Name: "analytics_data_points_total",
                Help: "Total number of data points in analytics",
            },
        ),
        eventCount: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "analytics_events_total",
                Help: "Total number of collected events",
            },
            []string{"action"},
        ),
    }
}
```

## ğŸ”§ HTTPå¤„ç†å™¨

### 1. **åˆ†æAPIç«¯ç‚¹**

```go
func (ac *AnalyticsController) GetAnalytics(c *gin.Context) {
    var req AnalyticsRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(400, gin.H{
            "success": false,
            "message": "è¯·æ±‚å‚æ•°é”™è¯¯",
            "error":   err.Error(),
        })
        return
    }

    // éªŒè¯æƒé™
    userID := ac.getUserIDFromContext(c)
    if !ac.hasAnalyticsPermission(userID) {
        c.JSON(403, gin.H{
            "success": false,
            "message": "æ— æƒé™è®¿é—®åˆ†ææ•°æ®",
        })
        return
    }

    response, err := ac.analyticsService.GetAnalytics(c.Request.Context(), &req)
    if err != nil {
        ac.logger.Errorf("Failed to get analytics: %v", err)
        c.JSON(500, gin.H{
            "success": false,
            "message": "è·å–åˆ†ææ•°æ®å¤±è´¥",
        })
        return
    }

    c.Header("Cache-Control", "private, max-age=300") // 5åˆ†é’Ÿç¼“å­˜
    c.JSON(200, response)
}

func (ac *AnalyticsController) ExportData(c *gin.Context) {
    var req AnalyticsRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(400, gin.H{
            "success": false,
            "message": "è¯·æ±‚å‚æ•°é”™è¯¯",
        })
        return
    }

    format := c.DefaultQuery("format", "csv")
    
    data, err := ac.analyticsService.ExportData(c.Request.Context(), &req, format)
    if err != nil {
        ac.logger.Errorf("Failed to export data: %v", err)
        c.JSON(500, gin.H{
            "success": false,
            "message": "æ•°æ®å¯¼å‡ºå¤±è´¥",
        })
        return
    }

    filename := fmt.Sprintf("analytics_%s.%s", time.Now().Format("20060102"), format)
    c.Header("Content-Disposition", fmt.Sprintf("attachment; filename=%s", filename))
    
    switch format {
    case "csv":
        c.Header("Content-Type", "text/csv")
    case "json":
        c.Header("Content-Type", "application/json")
    case "excel":
        c.Header("Content-Type", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    }

    c.Data(200, c.GetHeader("Content-Type"), data)
}
```

## ğŸ“ æ€»ç»“

ç»Ÿè®¡åˆ†æåŠŸèƒ½ä¸ºMovieInfoé¡¹ç›®æä¾›äº†å…¨é¢çš„æ•°æ®æ´å¯Ÿèƒ½åŠ›ï¼š

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
1. **å¤šç»´åˆ†æ**ï¼šç”¨æˆ·è¡Œä¸ºã€å†…å®¹è´¨é‡ã€å¹³å°å¥åº·åº¦åˆ†æ
2. **å®æ—¶æ”¶é›†**ï¼šäº‹ä»¶é©±åŠ¨çš„å®æ—¶æ•°æ®æ”¶é›†æœºåˆ¶
3. **è¶‹åŠ¿åˆ†æ**ï¼šå†å²æ•°æ®è¶‹åŠ¿å’Œé¢„æµ‹åˆ†æ
4. **æ•°æ®å¯¼å‡º**ï¼šæ”¯æŒå¤šç§æ ¼å¼çš„æ•°æ®å¯¼å‡º

**æŠ€æœ¯ç‰¹æ€§**ï¼š
- é«˜æ€§èƒ½çš„æ—¶åºæ•°æ®å¤„ç†
- æ™ºèƒ½çš„æ•°æ®èšåˆç®—æ³•
- çµæ´»çš„æŸ¥è¯¢å’Œè¿‡æ»¤æœºåˆ¶
- å®Œå–„çš„ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

**ä¸šåŠ¡ä»·å€¼**ï¼š
- ç”¨æˆ·è¡Œä¸ºæ´å¯Ÿ
- å†…å®¹è´¨é‡ç›‘æ§
- è¿è¥å†³ç­–æ”¯æŒ
- äº§å“ä¼˜åŒ–æŒ‡å¯¼

è‡³æ­¤ï¼Œè¯„è®ºæ‰“åˆ†æœåŠ¡çš„æ ¸å¿ƒåŠŸèƒ½å·²ç»å®Œæˆã€‚ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†ç»§ç»­å®Œæˆä¸»é¡µæœåŠ¡çš„å¼€å‘æ–‡æ¡£ã€‚

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å·²å®Œæˆ  
**æœ€åæ›´æ–°**: 2025-07-22  
**ä¸‹ä¸€æ­¥**: [ç¬¬41æ­¥ï¼šWebæœåŠ¡å™¨æ­å»º](../09-web-service/41-web-server.md)
